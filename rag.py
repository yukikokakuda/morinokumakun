
import os
import re
import hashlib
import shutil
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import streamlit as st
from dotenv import load_dotenv
from streamlit_chat import message

# LangChain
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

# BM25 ã¯ â€œã‚ã‚Œã°ä½¿ã†â€
try:
    from langchain_community.retrievers import BM25Retriever
    HAS_BM25 = True
except Exception:
    BM25Retriever = None  # å‹ãƒ’ãƒ³ãƒˆç”¨
    HAS_BM25 = False

# -----------------------------
# ãƒšãƒ¼ã‚¸è¨­å®šãƒ»å®šæ•°
# -----------------------------
st.set_page_config(page_title="Streamlit RAG Starter â€” LangChain + OpenAI + Chroma", layout="centered")

BASE_DIR = Path(__file__).parent.resolve()
RES_DIR = BASE_DIR / "resources"
HERO_IMG = BASE_DIR / "assets" / "myakujii.png"

# æ—¢å­˜DBãŒ1536æ¬¡å…ƒæƒ³å®šãªã®ã§ small ã‚’æ¡ç”¨ï¼ˆå¤‰ãˆã‚‹ãªã‚‰ä¿å­˜å…ˆã‚‚å¤‰ãˆã‚‹ï¼‰
EMBED_MODEL = "text-embedding-3-small"  # 1536 dims
VECTOR_DIR = RES_DIR / f"note_{EMBED_MODEL.replace('-', '_')}"
PERSIST_DIR = str(VECTOR_DIR)  # â† Chromaã«ã¯å¿…ãš str ã‚’æ¸¡ã™

CHUNK_SIZE = 900
CHUNK_OVERLAP = 180
K_VECT = 20
K_BM25 = 10
REL_THRESHOLD = 0.28

ACL_CHOICES = ["public", "finance", "engineering", "sales"]

# -----------------------------
# .env / ãƒ¢ãƒ‡ãƒ«
# -----------------------------
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    st.error("OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ç’°å¢ƒå¤‰æ•°ã¾ãŸã¯ .env ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    st.stop()

LLM_MODEL = "gpt-4.1-mini"  # é€Ÿåº¦ãƒ»äº’æ›æ€§é‡è¦–ã€‚

llm = ChatOpenAI(api_key=api_key, model=LLM_MODEL)
rewriter = ChatOpenAI(api_key=api_key, model="gpt-4.1-mini")  # ã‚¯ã‚¨ãƒªæ•´å½¢ç”¨

# -----------------------------
# UIãƒ˜ãƒƒãƒ€
# -----------------------------
# --- 2æ®µã‚¿ã‚¤ãƒˆãƒ« ---
st.markdown("""
<style>
h1.app-title{
  font-weight: 800;
  line-height: 1.15;
  margin: .2rem 0 .8rem;
  /* ç”»é¢å¹…ã«å¿œã˜ã¦ã‚µã‚¤ã‚ºèª¿æ•´ */
  font-size: clamp(2.2rem, 4vw + 1rem, 3.6rem);
}
.nowrap{ white-space: nowrap; }  /* LangChain / OpenAI / Chroma ãªã©ã‚’é€”ä¸­ã§æŠ˜ã‚Šè¿”ã•ãªã„ */
</style>

<h1 class="app-title">
  Streamlit RAG Starter <br/>
  <span class="nowrap">â€” LangChain + OpenAI + Chroma</span>
</h1>
""", unsafe_allow_html=True)
mid = st.columns([1, 2, 1])[1]
with mid:
    if HERO_IMG.exists():
        st.image(str(HERO_IMG), width=280)
st.markdown(
    """
<div style="text-align:center; font-weight:700; margin: 8px 0 18px; font-size:1.05rem;">
ç§ã¯ãƒŸãƒ£ã‚¯ã˜ãƒï¼ä¸–ã®ä¸­ã€ã ã‚Œã‚‚çŸ¥ã‚‰ãªã„æ¶ç©ºã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã˜ã‚ƒã€‚ChatGPTã«èã„ã¦ã‚‚ã€çµ¶å¯¾ã«ãƒ¯ã‚·ã®ã“ã¨ã¯çŸ¥ã‚‰ãªã„ã¯ãšã€‚<br>
ã“ã®ã‚¢ãƒ—ãƒªã«ã®ã¿ã€ãƒŸãƒ£ã‚¯ã˜ãƒã«é–¢ã™ã‚‹æƒ…å ±ãŒRAGã¨ã—ã¦ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ã€‚ã‚‚ã—RAGã®å‡„ã•ã‚’ä½“é¨“ã—ãŸã‘ã‚Œã°ã€ãƒŸãƒ£ã‚¯ã˜ãƒã«ã¤ã„ã¦ã€è³ªå•ã™ã‚‹ã®ã˜ã‚ƒï¼Tech0ã®æ—…è²»è¦ç¨‹ï¼ˆæ¶ç©ºï¼‰ã‚‚ç™»éŒ²ã—ã¦ã„ã‚‹ãï¼
</div>
""",
    unsafe_allow_html=True,
)

# -----------------------------
# ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼ˆé‹ç”¨ã£ã½ã„åˆ¶å¾¡ï¼‰
# -----------------------------
st.sidebar.subheader("ãƒœãƒƒãƒˆè¨­å®š")
user_acl = st.sidebar.multiselect("ã‚ãªãŸã®æ¨©é™ï¼ˆACLï¼‰", ACL_CHOICES, default=["public"])
mode = st.sidebar.selectbox("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ–¹é‡", ["å®‰å…¨è¨­è¨ˆ (aâ†’câ†’b)", "ä½¿ã„å‹æ‰‹é‡è¦– (aâ†’bâ†’c)"])
allow_general = st.sidebar.toggle("RAGã«ç„¡ã„ã¨ãä¸€èˆ¬å›ç­”ã‚‚è¨±å¯ã™ã‚‹ï¼ˆbï¼‰", value=False)
st.sidebar.caption("a: æ˜ç¢ºåŒ–è³ªå• / b: ä¸€èˆ¬å›ç­” / c: ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")

# å†æ§‹ç¯‰ãƒœã‚¿ãƒ³
if st.sidebar.button("ğŸ” ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†æ§‹ç¯‰", help="resourcesé…ä¸‹ã‚’å†èª­ã¿è¾¼ã¿ã—ã¦ãƒ™ã‚¯ãƒˆãƒ«DBã‚’ä½œã‚Šç›´ã—ã¾ã™"):
    st.cache_resource.clear()
    shutil.rmtree(VECTOR_DIR, ignore_errors=True)
    st.session_state["_reindexed"] = True
    st.rerun()
if st.session_state.get("_reindexed"):
    st.sidebar.success("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å†æ§‹ç¯‰ã—ã¾ã—ãŸã€‚")
    st.session_state["_reindexed"] = False

# --- å±¥æ­´ã‚¯ãƒªã‚¢ï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼‰ ---
st.sidebar.divider()
if st.sidebar.button("ğŸ§¹ ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ã‚¯ãƒªã‚¢", use_container_width=True,
                     help="ä¼šè©±ãƒ­ã‚°ã‚’æ¶ˆã—ã¦æœ€åˆã‹ã‚‰ã‚„ã‚Šç›´ã—ã¾ã™"):
    st.session_state.pop("messages", None)  # â† å±¥æ­´ã‚’æ¶ˆã™
    st.sidebar.success("å±¥æ­´ã‚’æ¶ˆå»ã—ã¾ã—ãŸã€‚")
    st.rerun()  # ç”»é¢ã‚’å†æç”»

# -----------------------------
# èª­ã¿è¾¼ã¿ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# -----------------------------
def _list_source_paths() -> List[Path]:
    # å¿…è¦ã«ãªã‚Œã° *.md, *.pdf ã‚’è¿½åŠ 
    return sorted((RES_DIR).glob("*.txt"))

def _sanitize_meta(meta: dict) -> dict:
    # Chromaã®ãƒ¡ã‚¿ã¯ str/int/float/bool/None ã®ã¿
    clean = {}
    for k, v in (meta or {}).items():
        if isinstance(v, (str, int, float, bool)) or v is None:
            clean[k] = v
        else:
            clean[k] = str(v)
    return clean

def _load_and_chunk(paths: List[Path]) -> List[Document]:
    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    out: List[Document] = []
    for p in paths:
        tdocs = TextLoader(str(p), encoding="utf-8").load()
        splits = splitter.split_documents(tdocs)
        for i, d in enumerate(splits):
            d.metadata.update(
                _sanitize_meta(
                    {
                        "source": p.name,     # ä¾‹: tech0_travel_policy.txt
                        "title": p.stem,      # ä¾‹: tech0_travel_policy
                        "chunk": i,
                        "acl": "public",      # æ–‡å­—åˆ—ã§ä¿å­˜ï¼ˆlistç¦æ­¢ï¼‰
                        "domain": "internal-policy",
                    }
                )
            )
        out.extend(splits)
    return out

# -----------------------------
# ã‚¹ãƒˆã‚¢æ§‹ç¯‰ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
# -----------------------------
@st.cache_resource(show_spinner=False)
def build_indices() -> Tuple[Any, Optional[Any], List[Document]]:
    embeddings = OpenAIEmbeddings(api_key=api_key, model=EMBED_MODEL)

    def rebuild() -> Tuple[Any, List[Document]]:
        paths = _list_source_paths()
        if not paths:
            raise FileNotFoundError(f"{RES_DIR} ã« *.txt ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        docs_ = _load_and_chunk(paths)
        if not docs_:
            raise ValueError("èª­ã¿è¾¼ã‚“ã ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒ0ä»¶ã§ã™ã€‚ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹/æ–‡å­—ã‚³ãƒ¼ãƒ‰ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        vs_ = Chroma.from_documents(documents=docs_, embedding=embeddings, persist_directory=PERSIST_DIR)
        return vs_, docs_

    if VECTOR_DIR.exists():
        vs = Chroma(embedding_function=embeddings, persist_directory=PERSIST_DIR)
        try:
            n = vs._collection.count()
        except Exception:
            n = 0
        if n == 0:
            shutil.rmtree(VECTOR_DIR, ignore_errors=True)
            vs, docs = rebuild()
        else:
            raw = vs._collection.get(include=["metadatas", "documents"])
            docs = [
                Document(page_content=c, metadata=(m or {}))
                for c, m in zip(raw.get("documents", []), raw.get("metadatas", []))
            ]
            if not docs:
                shutil.rmtree(VECTOR_DIR, ignore_errors=True)
                vs, docs = rebuild()
    else:
        vs, docs = rebuild()

    bm25: Optional[Any] = None
    if HAS_BM25 and docs:
        bm25 = BM25Retriever.from_documents(docs)
        bm25.k = K_BM25

    return vs, bm25, docs

VS, BM25, ALL_DOCS = build_indices()

# è¦‹ãˆã‚‹åŒ–
st.sidebar.caption(f"Persist dir: {PERSIST_DIR}")
st.sidebar.caption(f"Indexed files: {', '.join(sorted({d.metadata.get('source') for d in ALL_DOCS}))}")
st.sidebar.caption(f"Chunks: {len(ALL_DOCS)}")

# -----------------------------
# æ¤œç´¢ç³»
# -----------------------------
def domain_router(q: str) -> str:
    internal_kw = ["ãƒŸãƒ£ã‚¯ã˜ãƒ", "ã²ã‚ã˜ãƒ", "RAG", "ãƒ™ã‚¯ã‚¿", "Chroma", "è¬›ç¾©", "ãƒ‡ãƒ¢",
                   "Tech0", "æ—…è²»", "æ—…è²»è¦ç¨‹", "å‡ºå¼µ", "å®¿æ³Š", "æ—¥å½“"]
    if any(k.lower() in q.lower() for k in internal_kw):
        return "internal"
    both_kw = ["è¨­è¨ˆ", "è¦ä»¶", "ä»•æ§˜", "ç¤¾å†…", "æ‰‹é †", "è¦ç¨‹"]
    if any(k in q for k in both_kw):
        return "both"
    return "general"

def normalize_query(q: str) -> str:
    q = q.replace("ã€€", " ").strip()
    return re.sub(r"\s+", " ", q)

def condense_query(history: List[Dict], q: str) -> str:
    last = "\n".join([f"{'ãƒ¦ãƒ¼ã‚¶ãƒ¼' if m['role']=='user' else 'ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ'}: {m['content']}" for m in history[-6:]])
    prompt = ChatPromptTemplate.from_messages([
        ("system",
         "ã‚ãªãŸã¯æ¤œç´¢ã‚¯ã‚¨ãƒªæ•´å½¢ã®å°‚é–€å®¶ã§ã™ã€‚"
         "ã€çµ¶å¯¾æ¡ä»¶ã€‘å›ºæœ‰åè©ãƒ»ç¤¾å†…ç”¨èªï¼ˆä¾‹: ãƒŸãƒ£ã‚¯ã˜ãƒ/Tech0 ç­‰ï¼‰ã¯å¿…ãšãã®ã¾ã¾æ®‹ã™ã€‚"
         "çœç•¥ãƒ»è¨€ã„æ›ãˆãƒ»å‰Šé™¤ã¯ã—ãªã„ã€‚æ—¥æœ¬èª1æ–‡ã€çŸ­ãã€‚å‡ºåŠ›ã¯ã‚¯ã‚¨ãƒªã®ã¿ã€‚"),
        ("human", "å±¥æ­´:\n{h}\n\nç›´è¿‘è³ªå•:\n{q}")
    ])
    out = rewriter.invoke(prompt.format_messages(h=last, q=q)).content.strip()
    return out or q


def expand_queries(q: str, original: str) -> List[str]:
    seeds = [original]  # â†åŸæ–‡ã‚’æœ€å„ªå…ˆã§ä¿æŒ
    # å›ºæœ‰åè©ã®æ‰‹ä½œã‚Šè¿½åŠ ï¼ˆè½ã¡ãªã„ä¿é™ºï¼‰
    if "ãƒŸãƒ£ã‚¯" in original:
        seeds += ["ãƒŸãƒ£ã‚¯ã˜ãƒ ã¨ã¯", "ãƒŸãƒ£ã‚¯ã˜ãƒ ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼", "ãƒŸãƒ£ã‚¯ã˜ãƒ èª¬æ˜"]
    p = ChatPromptTemplate.from_messages([
        ("system", "ä¸ãˆãŸã‚¯ã‚¨ãƒªã«å¯¾ã—æ„å‘³ã®ç•°ãªã‚‹æ—¥æœ¬èªè¨€ã„æ›ãˆã‚’3ã¤ã€æ”¹è¡Œã§ã€‚èª¬æ˜ä¸è¦ã€‚"),
        ("human", "{q}")
    ])
    out = rewriter.invoke(p.format_messages(q=q)).content.strip()
    seeds += [s.strip(" ãƒ»-â€¢\n") for s in out.splitlines() if s.strip()]
    # é‡è¤‡é™¤å»
    seen, ret = set(), []
    for s in seeds:
        if s not in seen:
            seen.add(s); ret.append(s)
    return ret[:4]



def uniq_id(doc: Document) -> str:
    key = f"{doc.metadata.get('source','')}-{doc.metadata.get('chunk','')}"
    return key + "-" + hashlib.md5(doc.page_content[:200].encode("utf-8")).hexdigest()

def _allow(meta_acl: Optional[str], user_acl_list: List[str]) -> bool:
    if meta_acl is None:
        return True
    return str(meta_acl) in set(user_acl_list)

def hybrid_retrieve(queries: List[str], acl: List[str]) -> List[Tuple[Document, float, str]]:
    pool: Dict[str, Tuple[Document, float, str]] = {}

    # ---- ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆChromaã®filterã¯ä½¿ã‚ãšã€Pythonå´ã§ACLãƒã‚§ãƒƒã‚¯ï¼‰----
    for q in queries:
        docs_scores = VS.similarity_search_with_relevance_scores(q, k=K_VECT)
        for d, s in docs_scores:
            if not _allow(d.metadata.get("acl"), acl):
                continue
            uid = uniq_id(d)
            if uid not in pool and s >= REL_THRESHOLD:
                pool[uid] = (d, float(s), "vect")

    # ---- BM25ï¼ˆã‚ã‚‹å ´åˆã®ã¿ï¼‰----
    if BM25:
        for q in queries:
            for d in BM25.get_relevant_documents(q):
                if not _allow(d.metadata.get("acl"), acl):
                    continue
                uid = uniq_id(d)
                if uid not in pool:
                    pool[uid] = (d, 0.30, "bm25")
                else:
                    old = pool[uid]
                    pool[uid] = (old[0], max(old[1], 0.40), old[2])

    return sorted(pool.values(), key=lambda x: x[1], reverse=True)[:8]

SYSTEM_RAG = """ã‚ãªãŸã¯ä¼æ¥­å†…RAGã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸ context ã ã‘ã‚’æ ¹æ‹ ã«æ—¥æœ¬èªã§ç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„ã€‚
- contextã«ååˆ†ãªæ ¹æ‹ ãŒãªã„å ´åˆã¯ã€æ¨æ¸¬ã›ãšã€Œã“ã®RAGã§ã¯è©²å½“æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚
- ç¤¾å¤–æƒ…å ±ã‚„ä¸€èˆ¬çŸ¥è­˜ã‚’æ··ãœãªã„ã§ãã ã•ã„ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯åˆ¥ã§å‡¦ç†ã—ã¾ã™ï¼‰ã€‚
- ç®‡æ¡æ›¸ãæ­“è¿ã€‚å„ä¸»å¼µã®å¾Œã« [1], [2] ã®ã‚ˆã†ã«å‚ç…§ç•ªå·ã‚’ä»˜ã‘ã€æœ€å¾Œã«ã€Œå‚è€ƒ: ã‚¿ã‚¤ãƒˆãƒ«#ãƒãƒ£ãƒ³ã‚¯â€¦ã€ã‚’åˆ—æŒ™ã—ã¦ãã ã•ã„ã€‚
"""
PROMPT_RAG = ChatPromptTemplate.from_messages(
    [
        ("system", SYSTEM_RAG),
        ("human", "context:\n{context}\n\nè³ªå•: {q}"),
    ]
)

SYSTEM_GENERAL = """ã‚ãªãŸã¯æœ‰èƒ½ãªæ—¥æœ¬èªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚äº‹å®Ÿã«åŸºã¥ãã€ç°¡æ½”ã‹ã¤ä¸å¯§ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚
ãƒŸãƒ£ã‚¯ã˜ãƒï¼ˆç¤¾å†…å›ºæœ‰ï¼‰ã®æƒ…å ±ã¯ä½¿ã‚ãšã€ä¸€èˆ¬çš„ãªèª¬æ˜ã®ã¿ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚å¿…è¦ãªã‚‰æ³¨æ„ç‚¹ã‚‚æ·»ãˆã¦ãã ã•ã„ã€‚"""
PROMPT_GENERAL = ChatPromptTemplate.from_messages([("system", SYSTEM_GENERAL), ("human", "{q}")])

def compose_with_citations(q: str, docs: List[Tuple[Document, float, str]]) -> Tuple[str, float, str]:
    if not docs:
        return "", 0.0, ""
    refs, parts = [], []
    for i, (d, s, src) in enumerate(docs, start=1):
        parts.append(f"[{i}] {d.page_content}")
        title = d.metadata.get("title", d.metadata.get("source", "doc"))
        refs.append(f"[{i}] {title}#{d.metadata.get('chunk','')}")
    context = "\n\n".join(parts)
    msgs = PROMPT_RAG.format_messages(context=context, q=q)
    out = llm.invoke(msgs).content
    max_rel = max(score for _, score, _ in docs)
    return out, float(max_rel), "å‚è€ƒ: " + ", ".join(refs)

def confidence_check(text: str) -> float:
    p = ChatPromptTemplate.from_messages(
        [
            ("system", "ã‚ãªãŸã¯å›ç­”ã®è‡ªå·±è©•ä¾¡å™¨ã§ã™ã€‚ä»¥ä¸‹ã®å›ç­”ãŒã€ä¸ãˆã‚‰ã‚ŒãŸç¤¾å†…ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã ã‘ã§ååˆ†ã«è£ä»˜ã‘ã‚‰ã‚Œã¦ã„ã‚‹ã‹ã‚’0ã€œ1ã§å‡ºåŠ›ã€‚æ•°å€¤ã®ã¿ã€‚"),
            ("human", "{ans}"),
        ]
    )
    try:
        s = llm.invoke(p.format_messages(ans=text)).content.strip()
        m = re.findall(r"1(?:\.0+)?|0\.\d+|0", s)
        return max(0.0, min(1.0, float(m[0]))) if m else 0.5
    except Exception:
        return 0.5

def fallback_strategy(q: str, policy: str, allow_general_flag: bool) -> str:
    clarify = "ã‚‚ã†å°‘ã—å…·ä½“çš„ã«æ•™ãˆã¦ãã ã•ã„ã€‚ï¼ˆä¾‹ï¼šå¯¾è±¡ç¯„å›²ï¼æ™‚ç‚¹ï¼ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰"
    escalate = "ç¤¾å†…çª“å£ã«ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼š#help-desk / æ‹…å½“: support@example.comï¼‰ã€‚"
    general = (
        llm.invoke(PROMPT_GENERAL.format_messages(q=q)).content
        if allow_general_flag
        else "ä¸€èˆ¬å›ç­”ã¯ç¾åœ¨ã‚ªãƒ•ã«ãªã£ã¦ã„ã¾ã™ï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ã§æœ‰åŠ¹åŒ–ã§ãã¾ã™ï¼‰ã€‚"
    )
    if policy.startswith("å®‰å…¨è¨­è¨ˆ"):
        return f"{clarify}\n\n{escalate}\n\n{general if allow_general_flag else ''}".strip()
    else:
        return f"{clarify}\n\n{general}\n\n{escalate}".strip()

# -----------------------------
# ã‚»ãƒƒã‚·ãƒ§ãƒ³
# -----------------------------
if "messages" not in st.session_state:
    st.session_state.messages = []  # {'role': 'user'|'assistant', 'content': str}

# æ—¢å­˜ãƒ­ã‚°è¡¨ç¤ºï¼ˆå³=ãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼å·¦=ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆï¼‰
for i, m in enumerate(st.session_state.messages):
    if m["role"] == "user":
        message(m["content"], is_user=True, avatar_style="personas", seed="hiroji", key=f"msg_{i}_user")
    else:
        message(m["content"], is_user=False, avatar_style="bottts", seed="myakuji", key=f"msg_{i}_bot")

# -----------------------------
# å…¥åŠ› â†’ å…¸å‹ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œ
# -----------------------------
q_raw = st.chat_input("èããŸã„ã“ã¨ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:")
if q_raw:
    st.session_state.messages.append({"role": "user", "content": q_raw})
    with st.spinner("æ¤œç´¢ï¼†ç”Ÿæˆä¸­â€¦"):
        domain = domain_router(q_raw)
        q_norm = normalize_query(q_raw)
        q_condensed = condense_query(st.session_state.messages, q_norm)
        queries = expand_queries(q_condensed, original=q_norm)  # â†åŸæ–‡ã‚’æ¸¡ã™

        answer_text = ""
        citations = ""
        conf = 0.0

        if domain in ("internal", "both"):
            docs = hybrid_retrieve(queries, acl=user_acl)
            if docs:
                answer_text, relmax, citations = compose_with_citations(q_norm, docs)
                conf_llm = confidence_check(answer_text)
                conf = (relmax + conf_llm) / 2.0

        if not answer_text or conf < 0.35:
            if domain in ("general", "both"):
                if allow_general:
                    answer_text = llm.invoke(PROMPT_GENERAL.format_messages(q=q_norm)).content
                else:
                    answer_text = fallback_strategy(q_norm, policy=mode, allow_general_flag=allow_general)
            else:
                answer_text = fallback_strategy(q_norm, policy=mode, allow_general_flag=allow_general)

        if citations:
            answer_text = f"{answer_text}\n\n{citations}"

    st.session_state.messages.append({"role": "assistant", "content": answer_text})
    st.rerun()
